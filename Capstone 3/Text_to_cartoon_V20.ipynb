{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfde4ec2",
   "metadata": {},
   "source": [
    "# Generative AI: generating cartoons from text using NLP and image processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b4ab2",
   "metadata": {},
   "source": [
    "The tentative workflow for text to cartoon generative project is as follows:\n",
    "\n",
    "1) Collect and preprocess data: Gather a dataset of cartoons and their corresponding descriptions or captions. I have scraped online to gather a vast amount of cartoons. Image downloader youtube plugin was used to collect the data. The cartoons collected as such had their file name as the key and hence not much cleaning was necessary. \n",
    "\n",
    "2) Train an NLP model: There are multiple ways to deal with the text recognition. for example, we can take advantage of a pre-trained language model or train our own using a deep learning framework like TensorFlow or PyTorch. In this project we will be first try a pre-attention simple NLP model. Next we'll be using pre-trained models such as GPT or BARD using their API. \n",
    "\n",
    "3) Train a computer vision model: Again multiple ways to do it such as using a pre-trained image recognition model like VGG or Inception, or train our own model if we have a large labeled dataset. For this project we'll try a pre-trained model from Huggingface.\n",
    "\n",
    "4) Design an architecture that combines the textual and visual information.\n",
    "\n",
    "5) Evaluate and refine:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0651f8",
   "metadata": {},
   "source": [
    "# Step 1: Generate a text to cartoon algorithm the old-fashined way with limited associate words with images where matching words with image file names, that are image keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f246f64",
   "metadata": {},
   "source": [
    "#### For NLP, STANFORD's POS tagger would be used. File image names would be imported by OS. Word matching priorities: 1) Exact match 2) file name contains the word 3) Synonym using a dictionary (e.g. Wordnet) 4) no match- blank space "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082e931",
   "metadata": {},
   "source": [
    "#### For visualization, tkinter canvas was used. tkinter is a GUI framework that is built into python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91360eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assuming your images are stored in a directory\n",
    "image_directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'\n",
    "\n",
    "# Get the list of image file names\n",
    "image_files = os.listdir(image_directory)\n",
    "\n",
    "# Extract the labels from the file names\n",
    "labels = [file.split('.')[0] for file in image_files]\n",
    "\n",
    "# Print the labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Directory containing the images\n",
    "image_directory = r\"C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\"\n",
    "\n",
    "# List to store image-label associations\n",
    "image_label_mapping = {}\n",
    "\n",
    "# Iterate over the image files in the directory\n",
    "for file_name in os.listdir(image_directory):\n",
    "    # Construct the file path for the current image\n",
    "    image_path = os.path.join(image_directory, file_name)\n",
    "    \n",
    "    # Open the image using a suitable library like Pillow (PIL)\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Associate the image path with the file name (label) in the mapping dictionary\n",
    "    image_label_mapping[file_name] = image\n",
    "\n",
    "# Accessing an image based on its label\n",
    "#label = 'antisense-oligonucleotide-with-mrna.png'\n",
    "#image = image_label_mapping[label]\n",
    "# Now you can use the 'image' variable for further processing or display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accessing an image based on its label\n",
    "label = 'phosphate.png'\n",
    "image = image_label_mapping[label]\n",
    "\n",
    "# Display the image and its label\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.title(label)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e3c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize(\"phosphate is transferred from ATP to glucose\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "# Tokenize the text into sentences and words\n",
    "sentences = sent_tokenize(\"phosphate is transferred from ATP to glucose\")\n",
    "words = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag the words with part-of-speech\n",
    "tagged_words = [pos_tag(sent) for sent in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417eff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [word for sentence in tagged_words for (word, tag) in sentence if tag.startswith('NN')]\n",
    "verbs = [word for sentence in tagged_words for (word, tag) in sentence if tag.startswith('VBN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7326768",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bedb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df21c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '.png'\n",
    "\n",
    "nouns_mod = [item + suffix for item in nouns]\n",
    "verbs_mod = [item + suffix for item in verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0270e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image and its label\n",
    "for noun in nouns_mod:\n",
    "    label_POS = noun\n",
    "    image_POS = image_label_mapping[label_POS]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image_POS)\n",
    "    plt.title(label_POS)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "\n",
    "# List of labels for the items= nouns_mod\n",
    "image_names = ['phosphate.png', 'ATP.png']\n",
    "\n",
    "\n",
    "# List of image paths corresponding to the labels\n",
    "#image_paths = [image_label_mapping[label] for label in labels] (wrote by ChatGPT)\n",
    "\n",
    "\n",
    "# Create the Tkinter window\n",
    "#window = tk.Tk() chatgpt wrote it\n",
    "window = tk.Toplevel() \n",
    "# from stack overflow: https://stackoverflow.com/questions/20251161/tkinter-tclerror-image-pyimage3-doesnt-exist/20259317\n",
    "window.title(\"Item Animation\")\n",
    "\n",
    "# Create a canvas to display the images\n",
    "canvas = tk.Canvas(window, width=400, height=400)\n",
    "canvas.pack()\n",
    "\n",
    "# Load the images and display them sequentially\n",
    "image_objects = []\n",
    "for image_name in image_names:\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects.append(image)\n",
    "    # Attach image to a canvas item to prevent garbage collection\n",
    "    #canvas.image = image\n",
    "    \n",
    "    \n",
    "def animate_images(index):\n",
    "    canvas.delete(\"all\")  # Clear the canvas\n",
    "    canvas.create_image(200, 200, image=image_objects[index])  # Display the current image\n",
    "    canvas.after(2000, animate_images, (index + 1) % len(image_objects))  # Repeat after a delay\n",
    "\n",
    "# Start the animation\n",
    "animate_images(0)\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75cf31",
   "metadata": {},
   "source": [
    "#### This is a cartoon with a 'flicker' visualization where one item is replaced by the other item. I am more interested placing them side by side. Let's create the tkinter cartoon that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f529f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to show the cartoons side by side instead of one after another\n",
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "\n",
    "# List of image names\n",
    "image_names = ['phosphate.png', 'ATP.png']\n",
    "\n",
    "# Create the Tkinter window\n",
    "window_2 = tk.Toplevel()\n",
    "window_2.title(\"Item Animation\")\n",
    "\n",
    "# Create a canvas to display the images\n",
    "canvas_2 = tk.Canvas(window_2, width=800, height=400)\n",
    "canvas_2.pack()\n",
    "\n",
    "# Load the images and display them side by side\n",
    "image_objects_2 = []\n",
    "image_x_positions = [200, 800]  # X positions for the images\n",
    "for i, image_name in enumerate(image_names):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_2.append(image)\n",
    "    canvas_2.create_image(image_x_positions[i], 200, image=image)  # Display the image\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window_2.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0359dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 images in the canvas\n",
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "\n",
    "# List of image names\n",
    "image_names_3 = ['ATP.png', 'curving-arrow-thick-tapered-editable-6.png', 'phosphate.png']\n",
    "\n",
    "# Create the Tkinter window\n",
    "window_3 = tk.Toplevel()\n",
    "window_3.title(\"Item Animation\")\n",
    "\n",
    "# Create a canvas to display the images\n",
    "canvas_3 = tk.Canvas(window_3, width=800, height=400)\n",
    "canvas_3.pack()\n",
    "\n",
    "# Load the images and display them side by side\n",
    "image_objects_3 = []\n",
    "image_x_positions = [200, 400, 600]  # X positions for the images\n",
    "\n",
    "for i, image_name in enumerate(image_names_3):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_3.append(image)\n",
    "    canvas_3.create_image(image_x_positions[i], 200, image=image)  # Display the image\n",
    "\n",
    "''''path in image_directory:\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((190, 190))\n",
    "    images_2.append(ImageTk.PhotoImage(image))'''\n",
    "\n",
    "# Create image items on the canvas in a grid layout\n",
    "'''for i in range(2):\n",
    "    for j in range(2):\n",
    "        image_item = canvas_3.create_image((j * 200) + 100, (i * 200) + 100, image=image_objects_3[(i * 3) + j])'''\n",
    "        \n",
    "# Update the canvas to display the items\n",
    "canvas_3.update()\n",
    "\n",
    "# Run the Tkinter event loop to display the window\n",
    "window_3.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8665615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 images in the canvas\n",
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "\n",
    "# List of image names\n",
    "image_names_noun = nouns_mod\n",
    "image_names_verb = ['curving-arrow-thick-tapered-editable-6.png']\n",
    "\n",
    "# Create the Tkinter window\n",
    "window_3 = tk.Toplevel()\n",
    "window_3.title(\"Item Animation\")\n",
    "\n",
    "# Create a canvas to display the images\n",
    "canvas_3 = tk.Canvas(window_3, width=800, height=400)\n",
    "canvas_3.pack()\n",
    "\n",
    "# Load the images and display them side by side\n",
    "image_objects_3 = []\n",
    "image_x_positions = [200, 600]  # X positions for the images\n",
    "\n",
    "for i, image_name in enumerate(image_names_noun):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_3.append(image)\n",
    "    canvas_3.create_image(image_x_positions[i], 200, image=image)  # Display the image\n",
    "    \n",
    "for j, image_name in enumerate(image_names_verb):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_3.append(image)\n",
    "    canvas_3.create_image(400, 200, anchor= 'center', image=image)  # Display the image\n",
    "\n",
    "''''path in image_directory:\n",
    "    image = Image.open(path)\n",
    "    image = image.resize((190, 190))\n",
    "    images_2.append(ImageTk.PhotoImage(image))'''\n",
    "\n",
    "# Create image items on the canvas in a grid layout\n",
    "'''for i in range(2):\n",
    "    for j in range(2):\n",
    "        image_item = canvas_3.create_image((j * 200) + 100, (i * 200) + 100, image=image_objects_3[(i * 3) + j])'''\n",
    "        \n",
    "# Update the canvas to display the items\n",
    "canvas_3.update()\n",
    "\n",
    "# Run the Tkinter event loop to display the window\n",
    "window_3.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ad403",
   "metadata": {},
   "source": [
    "#### Now that we have figures out the cartoon basics. Let's now take user input and customize that in the output cartoon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b294b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = input('Step1?\\n')     # \\n ---> newline  ---> It causes a line break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d25a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_input = word_tokenize(input_1)\n",
    "#nltk.pos_tag(text_input)\n",
    "\n",
    "# Tokenize the text into sentences and words\n",
    "sentences_input = sent_tokenize(input_1)\n",
    "words_input = [word_tokenize(sent) for sent in sentences_input]\n",
    "\n",
    "# Tag the words with part-of-speech\n",
    "tagged_words_input = [pos_tag(sent) for sent in words]\n",
    "\n",
    "nouns_input = [word for sentences in tagged_words_input for (word, tag) in sentences if tag.startswith('NN')]\n",
    "verbs_input = [word for sentences in tagged_words_input for (word, tag) in sentences if tag.startswith('VBN')]\n",
    "\n",
    "suffix = '.png'\n",
    "\n",
    "nouns_input_mod = [item + suffix for item in nouns_input]\n",
    "verbs_input_mod = [item + suffix for item in verbs_input]\n",
    "\n",
    "print(nouns_input_mod, verbs_input_mod)\n",
    "\n",
    "#3 images in the canvas\n",
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "\n",
    "# List of image names\n",
    "image_names_noun_input = nouns_input_mod\n",
    "image_names_verb_input = ['curving-arrow-thick-tapered-editable-6.png']\n",
    "\n",
    "# Create the Tkinter window\n",
    "window_input = tk.Toplevel()\n",
    "window_input.title(\"Item Animation\")\n",
    "\n",
    "# Create a canvas to display the images\n",
    "canvas_input = tk.Canvas(window_input, width=800, height=400)\n",
    "canvas_input.pack()\n",
    "\n",
    "# Load the images and display them side by side\n",
    "image_objects_input = []\n",
    "image_x_positions = [200, 600]  # X positions for the images\n",
    "\n",
    "for i, image_name in enumerate(image_names_noun_input):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_input.append(image)\n",
    "    canvas_input.create_image(image_x_positions[i], 200, image=image)  # Display the image\n",
    "    \n",
    "for j, image_name in enumerate(image_names_verb):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_input.append(image)\n",
    "    canvas_input.create_image(400, 200, anchor= 'center', image=image)  # Display the image\n",
    "        \n",
    "# Update the canvas to display the items\n",
    "canvas_input.update()\n",
    "\n",
    "# Run the Tkinter event loop to display the window\n",
    "window_input.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734d18c",
   "metadata": {},
   "source": [
    "#### Use of synonym (soft match), instead of exact match from wordnet to generate the cartoon using wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "\n",
    "# Function to check if a file name matches the input text\n",
    "def is_exact_match(input_text, file_name):\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    file_name_without_extension = file_name_without_extension.lower()  # Convert to lowercase for matching\n",
    "    return input_text == file_name_without_extension\n",
    "\n",
    "# Function to get synonyms for a given word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to find the image path based on the input text\n",
    "def find_image_path(input_text, directory):\n",
    "    file_names = os.listdir(directory)\n",
    "    \n",
    "    # Check for exact matches\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(input_text, file_name):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    # Check for synonyms using WordNet\n",
    "    synonyms = get_synonyms(input_text)\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(file_name, input_text) or any(is_exact_match(synonym, file_name) for synonym in synonyms):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to display the cartoon image based on the user's input\n",
    "def display_cartoon():\n",
    "    input_text = entry.get()\n",
    "    input_text = input_text.lower()  # Convert the input text to lowercase for matching\n",
    "\n",
    "    directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'  # Replace with your image directory path\n",
    "\n",
    "    # Find the image path based on the input text\n",
    "    image_path = find_image_path(input_text, directory)\n",
    "\n",
    "    # Display the image if found\n",
    "    if image_path:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((200, 200))  # Adjust the size as needed\n",
    "        image_tk = ImageTk.PhotoImage(image)\n",
    "        canvas.create_image(200, 200, image=image_tk)\n",
    "        canvas.image = image_tk  # Store a reference to avoid garbage collection\n",
    "    else:\n",
    "        # Display a default image or show an error message\n",
    "        pass\n",
    "\n",
    "# Create a canvas to display the image\n",
    "canvas = tk.Canvas(window, width=400, height=400)\n",
    "canvas.pack()\n",
    "\n",
    "# Create an entry widget for the user to input text\n",
    "entry = tk.Entry(window)\n",
    "entry.pack()\n",
    "\n",
    "# Create a button to trigger the display of the cartoon image\n",
    "button = tk.Button(window, text=\"Display\", command=display_cartoon)\n",
    "button.pack()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "\n",
    "# Function to check if a file name matches the input text\n",
    "def is_exact_match(input_text, file_name):\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    file_name_without_extension = file_name_without_extension.lower()  # Convert to lowercase for matching\n",
    "    return input_text == file_name_without_extension\n",
    "\n",
    "# Function to get synonyms for a given word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to find the image path based on the input text\n",
    "def find_image_path(input_text, directory):\n",
    "    file_names = os.listdir(directory)\n",
    "    \n",
    "    # Check for exact matches\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(input_text, file_name):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    # Check for synonyms using WordNet\n",
    "    synonyms = get_synonyms(input_text)\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(file_name, input_text) or any(is_exact_match(synonym, file_name) for synonym in synonyms):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to display the cartoon image based on the user's input\n",
    "def display_cartoon():\n",
    "    input_text = entry.get()\n",
    "    input_text = input_text.lower()  # Convert the input text to lowercase for matching\n",
    "\n",
    "    directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'  # Replace with your image directory path\n",
    "\n",
    "    # Find the image path based on the input text\n",
    "    image_path = find_image_path(input_text, directory)\n",
    "\n",
    "    # Display the image if found\n",
    "    if image_path:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((200, 200))  # Adjust the size as needed\n",
    "        image_tk = ImageTk.PhotoImage(image)\n",
    "        canvas.create_image(200, 200, image=image_tk)\n",
    "        canvas.image = image_tk  # Store a reference to avoid garbage collection\n",
    "    else:\n",
    "        # Display a default image or show an error message\n",
    "        pass\n",
    "\n",
    "# Create a canvas to display the image\n",
    "canvas = tk.Canvas(window, width=400, height=400)\n",
    "canvas.pack()\n",
    "\n",
    "# Create an entry widget for the user to input text\n",
    "entry = tk.Entry(window)\n",
    "entry.pack()\n",
    "\n",
    "# Create a button to trigger the display of the cartoon image\n",
    "button = tk.Button(window, text=\"Display\", command=display_cartoon)\n",
    "button.pack()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()\n",
    "\n",
    "# Load the images and display them side by side\n",
    "image_objects_input = []\n",
    "image_x_positions = [200, 600]  # X positions for the images\n",
    "\n",
    "for i, image_name in enumerate(image_names_noun_input):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_input.append(image)\n",
    "    canvas_input.create_image(image_x_positions[i], 200, image=image)  # Display the image\n",
    "    \n",
    "for j, image_name in enumerate(image_names_verb):\n",
    "    image_path = os.path.join(image_directory, image_name)\n",
    "    image = ImageTk.PhotoImage(Image.open(image_path))\n",
    "    image_objects_input.append(image)\n",
    "    canvas_input.create_image(400, 200, anchor= 'center', image=image)  # Display the image\n",
    "        \n",
    "# Update the canvas to display the items\n",
    "canvas_input.update()\n",
    "\n",
    "# Run the Tkinter event loop to display the window\n",
    "window_input.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6eb283",
   "metadata": {},
   "source": [
    "#### Side by side cartoons: The arrows need to be defined based on from or to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b987c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "\n",
    "# Function to check if a file name matches the input text\n",
    "def is_exact_match(input_text, file_name):\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    file_name_without_extension = file_name_without_extension.lower()  # Convert to lowercase for matching\n",
    "    return input_text == file_name_without_extension\n",
    "\n",
    "# Function to get synonyms for a given word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to find the image path based on the input text\n",
    "def find_image_path(input_text, directory):\n",
    "    file_names = os.listdir(directory)\n",
    "\n",
    "    # Check for exact matches\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(input_text, file_name):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    # Check for synonyms using WordNet\n",
    "    synonyms = get_synonyms(input_text)\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(file_name, input_text) or any(is_exact_match(synonym, file_name) for synonym in synonyms):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to display the cartoon images based on the user's input\n",
    "def display_cartoon():\n",
    "    input_text = entry.get()\n",
    "    input_text = input_text.lower()  # Convert the input text to lowercase for matching\n",
    "\n",
    "    directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'  # Replace with your image directory path\n",
    "\n",
    "    # Tokenize the input text into individual words\n",
    "    tokens = nltk.word_tokenize(input_text)\n",
    "\n",
    "    # Clear the frame before displaying the new images\n",
    "    for widget in frame.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "    # Iterate through each word and display the corresponding cartoon image\n",
    "    x = 0  # Initial x-coordinate for image display\n",
    "    for word in tokens:\n",
    "        # Find the image path based on the current word\n",
    "        image_path = find_image_path(word, directory)\n",
    "\n",
    "        # Display the image if found\n",
    "        if image_path:\n",
    "            image = Image.open(image_path)\n",
    "            image = image.resize((100, 100))  # Adjust the size as needed\n",
    "            image_tk = ImageTk.PhotoImage(image)\n",
    "            label = tk.Label(frame, image=image_tk)\n",
    "            label.image = image_tk  # Store a reference to avoid garbage collection\n",
    "            label.pack(side=\"left\", padx=10)\n",
    "            x += 1\n",
    "\n",
    "# Create a frame to display the cartoon images\n",
    "frame = tk.Frame(window)\n",
    "frame.pack()\n",
    "\n",
    "# Create an entry widget for the user to input text\n",
    "entry = tk.Entry(window)\n",
    "entry.pack()\n",
    "\n",
    "# Create a button to trigger the display of the cartoon images\n",
    "button = tk.Button(window, text=\"Display\", command=display_cartoon)\n",
    "button.pack()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be108544",
   "metadata": {},
   "source": [
    "#### Soft match with the figure name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c992555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "\n",
    "# Function to check if a file name matches the input text\n",
    "def is_exact_match(input_text, file_name):\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    file_name_without_extension = file_name_without_extension.lower()  # Convert to lowercase for matching\n",
    "    return input_text in file_name_without_extension # in instead of ==\n",
    "\n",
    "# Function to get synonyms for a given word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to find the image path based on the input text\n",
    "def find_image_path(input_text, directory):\n",
    "    file_names = os.listdir(directory)\n",
    "\n",
    "    # Check for exact matches\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(input_text, file_name):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    # Check for synonyms using WordNet\n",
    "    synonyms = get_synonyms(input_text)\n",
    "    for file_name in file_names:\n",
    "        if is_exact_match(file_name, input_text) or any(is_exact_match(synonym, file_name) for synonym in synonyms):\n",
    "            return os.path.join(directory, file_name)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to display the cartoon images based on the user's input\n",
    "def display_cartoon():\n",
    "    input_text = entry.get()\n",
    "    input_text = input_text.lower()  # Convert the input text to lowercase for matching\n",
    "\n",
    "    directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'  # Replace with your image directory path\n",
    "\n",
    "    # Tokenize the input text into individual words\n",
    "    tokens = nltk.word_tokenize(input_text)\n",
    "\n",
    "    # Clear the frame before displaying the new images\n",
    "    for widget in frame.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "    # Iterate through each word and display the corresponding cartoon image\n",
    "    x = 0  # Initial x-coordinate for image display\n",
    "    for word in tokens:\n",
    "        # Find the image path based on the current word\n",
    "        image_path = find_image_path(word, directory)\n",
    "\n",
    "        # Display the image if found\n",
    "        if image_path:\n",
    "            image = Image.open(image_path)\n",
    "            image = image.resize((100, 100))  # Adjust the size as needed\n",
    "            image_tk = ImageTk.PhotoImage(image)\n",
    "            label = tk.Label(frame, image=image_tk)\n",
    "            label.image = image_tk  # Store a reference to avoid garbage collection\n",
    "            label.pack(side=\"left\", padx=10)\n",
    "            x += 1\n",
    "\n",
    "# Create a frame to display the cartoon images\n",
    "frame = tk.Frame(window)\n",
    "frame.pack()\n",
    "\n",
    "# Create an entry widget for the user to input text\n",
    "entry = tk.Entry(window)\n",
    "entry.pack()\n",
    "\n",
    "# Create a button to trigger the display of the cartoon images\n",
    "button = tk.Button(window, text=\"Display\", command=display_cartoon)\n",
    "button.pack()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266633d3",
   "metadata": {},
   "source": [
    "#### Priority based: exact match first priority and then 'in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71adfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "\n",
    "# Function to check if a file name matches the input text\n",
    "def is_exact_match(input_text, file_name):\n",
    "    file_name_without_extension = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "    file_name_without_extension = file_name_without_extension.lower()  # Convert to lowercase for matching\n",
    "    return input_text in file_name_without_extension # in instead of ==\n",
    "\n",
    "# Function to get synonyms for a given word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Function to find the image path based on the input text\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def find_image_path(input_text, directory):\n",
    "    file_names = os.listdir(directory)\n",
    "\n",
    "    # First round: Exact matches\n",
    "    exact_matches = []\n",
    "    partial_matches = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        # Remove the file extension\n",
    "        file_name_without_extension = os.path.splitext(file_name)[0]\n",
    "        file_name_without_extension = file_name_without_extension.lower()\n",
    "\n",
    "        # Check for exact matches\n",
    "        if input_text == file_name_without_extension:\n",
    "            exact_matches.append(file_name)\n",
    "        # Check for partial matches\n",
    "        elif input_text in file_name_without_extension:\n",
    "            partial_matches.append(file_name)\n",
    "\n",
    "    # Check exact matches\n",
    "    if exact_matches:\n",
    "        return os.path.join(directory, exact_matches[0])  # Return the first exact match\n",
    "\n",
    "    # Check partial matches if no exact match is found\n",
    "    if partial_matches:\n",
    "        return os.path.join(directory, partial_matches[0])  # Return the first partial match\n",
    "\n",
    "    # Third round: WordNet synonyms\n",
    "    synonyms = wordnet.synsets(input_text)\n",
    "    for synset in synonyms:\n",
    "        for lemma in synset.lemmas():\n",
    "            lemma_name = lemma.name().lower()\n",
    "            for file_name in file_names:\n",
    "                if lemma_name in file_name.lower():\n",
    "                    return os.path.join(directory, file_name)\n",
    "\n",
    "    return None\n",
    "\n",
    "# Function to display the cartoon images based on the user's input\n",
    "def display_cartoon():\n",
    "    input_text = entry.get()\n",
    "    input_text = input_text.lower()  # Convert the input text to lowercase for matching\n",
    "\n",
    "    directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'  # Replace with your image directory path\n",
    "\n",
    "    # Tokenize the input text into individual words\n",
    "    tokens = nltk.word_tokenize(input_text)\n",
    "\n",
    "    # Clear the frame before displaying the new images\n",
    "    for widget in frame.winfo_children():\n",
    "        widget.destroy()\n",
    "\n",
    "    # Iterate through each word and display the corresponding cartoon image\n",
    "    x = 0  # Initial x-coordinate for image display\n",
    "    for word in tokens:\n",
    "        # Find the image path based on the current word\n",
    "        image_path = find_image_path(word, directory)\n",
    "\n",
    "        # Display the image if found\n",
    "        if image_path:\n",
    "            image = Image.open(image_path)\n",
    "            image = image.resize((100, 100))  # Adjust the size as needed\n",
    "            image_tk = ImageTk.PhotoImage(image)\n",
    "            label = tk.Label(frame, image=image_tk)\n",
    "            label.image = image_tk  # Store a reference to avoid garbage collection\n",
    "            label.pack(side=\"left\", padx=10)\n",
    "            x += 1\n",
    "\n",
    "# Create a frame to display the cartoon images\n",
    "frame = tk.Frame(window)\n",
    "frame.pack()\n",
    "\n",
    "# Create an entry widget for the user to input text\n",
    "entry = tk.Entry(window)\n",
    "entry.pack()\n",
    "\n",
    "# Create a button to trigger the display of the cartoon images\n",
    "button = tk.Button(window, text=\"Display\", command=display_cartoon)\n",
    "button.pack()\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf988535",
   "metadata": {},
   "source": [
    "# Step 2: Using both pre-trained text and image models; to generate text-text or image-image similarity or text-image similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b87cb",
   "metadata": {},
   "source": [
    "## Text-text: Using Huggingface API for generating meaning from the text input before searching images from the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e66a483",
   "metadata": {},
   "source": [
    "In the cmd from Pytorch ##pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f499028",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e32115",
   "metadata": {},
   "source": [
    "On cmd install the packages #pip install torch torchvision ftfy regex\n",
    "#pip install ftfy regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "#import clip\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f89aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/openai/CLIP.git as clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision ftfy regex git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fb6ac",
   "metadata": {},
   "source": [
    "Cartoon to cartoon text: just as a trial to see if the packages are working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ATP to Phosphate to glucose-6-phosphate\"\n",
    "inputs = tokenizer.encode(text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37887103",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c959f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated Cartoon Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd054a9",
   "metadata": {},
   "source": [
    "#### Now this result is nonsensical. The transformer is clearly not finetuned. Hence a finetuned version is required.\n",
    "Hence, a biological text embedding is required. We could try to use 1) BIOBERT (pre-trained) or scrape 2) wikipedia or a 3) bioscience dictionary to generate the text embeddings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8721a52",
   "metadata": {},
   "source": [
    "#### BIOBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dmis-lab/biobert.git\n",
    "!cd biobert; pip install -r requirements.txt\n",
    "!pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0e1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and cataloging the functions for BIOBERT NER\n",
    "def term_extraction(text,out,type):\n",
    "\n",
    "  denotations=out['denotations']\n",
    "  relavent_terms=[]\n",
    "  \n",
    "  if type=='drug':\n",
    "    drug_terms=[]\n",
    "    for i in denotations:\n",
    "      if i['obj']== 'drug':\n",
    "        drug_terms.append((i['span']['begin'],i['span']['end']))\n",
    "   \n",
    "    for i in drug_terms:\n",
    "      start=i[0]\n",
    "      end=i[1]\n",
    "      relavent_terms.append(text[start:end])\n",
    "\n",
    "  if type=='disease':\n",
    "    species_terms=[]\n",
    "    for i in denotations:\n",
    "      if i['obj']== 'disease':\n",
    "        species_terms.append((i['span']['begin'],i['span']['end']))\n",
    "    \n",
    "    for i in species_terms:\n",
    "      start=i[0]\n",
    "      end=i[1]\n",
    "      relavent_terms.append(text[start:end])\n",
    "\n",
    "\n",
    "  if type=='species':\n",
    "    species_terms=[]\n",
    "    for i in denotations:\n",
    "      if i['obj']== 'species':\n",
    "        species_terms.append((i['span']['begin'],i['span']['end']))\n",
    "    \n",
    "    for i in species_terms:\n",
    "      start=i[0]\n",
    "      end=i[1]\n",
    "      relavent_terms.append(text[start:end])\n",
    "\n",
    "  return(relavent_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724cb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_raw(text, url=\"https://bern.korea.ac.kr/plain\"):\n",
    "  return requests.post(url, data={'sample_text': text}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"X-rays were negative and physical assessment determined soft tissue damage to the lateral aspect of her ankle. She was initially treated with ice, an ace wrap, crutches and mild pain medications (Tylenol with codeine)\"\n",
    "text2 = \"It is a skin disease causing much itchiness. Scratching leads to redness, swelling, cracking, weeping clear fluid, crusting, and scaling.\"\n",
    "out=query_raw(text)\n",
    "print(out)\n",
    "print(term_extraction(text,out,'drug'))\n",
    "print(term_extraction(text,out,'disease'))\n",
    "print(term_extraction(text,out,'species'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b3558",
   "metadata": {},
   "source": [
    "#### BIOBERT API is non-functional. And hence, we'll have to generate our own biological training. More on that later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35fe439",
   "metadata": {},
   "source": [
    "## Text to image: using CLIP and DALL-E. They both have built in text-text models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6e9c9",
   "metadata": {},
   "source": [
    "#### Before we try to associate texts to cartoon, let's see how a simple text to image genration API works. \n",
    "To do so, we need to use clip (image to text meaning generation) or DALL-E (text to image generation). First, we'll try clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b677796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import torchvision.transforms as T\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73854e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text and image inputs\n",
    "text = \"triple phosphate\"\n",
    "image_path = r\"C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\\ATP.png\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = PIL.Image.open(image_path).convert(\"RGB\")\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Tokenize the text input\n",
    "text_input = clip.tokenize([text]).to(device)\n",
    "\n",
    "# Encode the image and text inputs\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_input)\n",
    "\n",
    "# Perform the similarity calculation\n",
    "similarity_scores = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Display the similarity scores\n",
    "print(similarity_scores)\n",
    "\n",
    "# Display the cartoon image\n",
    "image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342c749",
   "metadata": {},
   "source": [
    "#### Using a known CLIP project as a positive control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61395957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A great example of how CLIP works: https://towardsdatascience.com/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# we initialize a tokenizer, image processor, and the model itself\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "urls=['https://images.unsplash.com/photo-1662955676669-c5d141718bfd?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=687&q=80',\n",
    "    'https://images.unsplash.com/photo-1552053831-71594a27632d?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=662&q=80',\n",
    "    'https://images.unsplash.com/photo-1530281700549-e82e7bf110d6?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=688&q=80']\n",
    "\n",
    "images=[Image.open(requests.get(i, stream=True).raw)  for i in urls]\n",
    "\n",
    "text_prompts=[\"a girl wearing a beanie\", \"a boy wearing a beanie\", \"a dog\", \"a dog at the beach\"]\n",
    "inputs = inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image \n",
    "probs = logits_per_image.softmax(dim=1) \n",
    "\n",
    "pd.DataFrame(probs.detach().numpy()*100, columns=text_prompts, index=list(['image1','image2', 'image3'])).style.background_gradient(axis=None,low=0, high=0.91).format(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61ff0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A great example of how CLIP works: https://towardsdatascience.com/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# we initialize a tokenizer, image processor, and the model itself\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "image1= r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\\ATP.png'\n",
    "image2= r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\\phosphate.png'\n",
    "images = [Image.open(image_path) for image_path in [image1, image2]]\n",
    "\n",
    "text_prompts=[\"a girl wearing a beanie\", \"a chemical structure\" ]\n",
    "inputs = inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image \n",
    "probs = logits_per_image.softmax(dim=1) \n",
    "\n",
    "pd.DataFrame(probs.detach().numpy()*100, columns=text_prompts, index=list(['image1','image2'])).style.background_gradient(axis=None,low=0, high=0.91).format(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying CLIP in current project\n",
    "\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# we initialize a tokenizer, image processor, and the model itself\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "image1= r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\\ATP.png'\n",
    "image2= r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\\phosphate.png'\n",
    "images = [Image.open(image_path) for image_path in [image1, image2]]\n",
    "\n",
    "text_prompts=[\"ATP\", \"Phosphate\" ]\n",
    "inputs = inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image \n",
    "probs = logits_per_image.softmax(dim=1) \n",
    "\n",
    "pd.DataFrame(probs.detach().numpy()*100, columns=text_prompts, index=list(['image1','image2'])).style.background_gradient(axis=None,low=0, high=0.91).format(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import os\n",
    "from PIL import Image\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# we initialize a tokenizer, image processor, and the model itself\n",
    "tokenizer = transformers.CLIPTokenizerFast.from_pretrained(model_id)\n",
    "processor = transformers.CLIPProcessor.from_pretrained(model_id)\n",
    "model = transformers.CLIPModel.from_pretrained(model_id).to(device)\n",
    "\n",
    "image_directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'\n",
    "\n",
    "image_files = [os.path.join(image_directory, file) for file in os.listdir(image_directory) if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "images = [Image.open(image_path) for image_path in image_files]\n",
    "\n",
    "text_prompts = [\"ATP\", \"Phosphate\"]\n",
    "\n",
    "inputs = processor(text=text_prompts, images=images, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "result_df = pd.DataFrame(probs.detach().numpy() * 100, columns=text_prompts, index=[os.path.basename(image_path) for image_path in image_files])\n",
    "result_df.style.background_gradient(axis=None, low=0, high=0.91).format(precision=2)\n",
    "\n",
    "#.detach explanation is here- https://stackoverflow.com/questions/63582590/why-do-we-call-detach-before-calling-numpy-on-a-pytorch-tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c22d41",
   "metadata": {},
   "source": [
    "#### Clearly, CLIP was able to identify the images. But it fails when we need more granular distinction between the images. Hence, for our purpose pre-trained CLIP would not be sufficient. Some of the other CLIP exampes are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a5b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load the CLIP model and pre-trained weights\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Set the directory path to your image library\n",
    "image_directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'\n",
    "\n",
    "# Get a list of image files in the directory\n",
    "image_files = [\n",
    "    os.path.join(image_directory, filename)\n",
    "    for filename in os.listdir(image_directory)\n",
    "    if filename.endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "]\n",
    "\n",
    "# Generate the image embeddings for all the images\n",
    "image_embeddings = []\n",
    "for image_file in image_files:\n",
    "    image = Image.open(image_file)\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(image_input)\n",
    "    image_embeddings.append(image_embedding)\n",
    "\n",
    "# Define the text input\n",
    "text = \"ATP to phosphate\"\n",
    "\n",
    "# Preprocess the text input\n",
    "text_inputs = torch.cat([clip.tokenize(text)]).to(device)\n",
    "\n",
    "# Calculate the similarity scores between the text input and image embeddings\n",
    "similarity_scores = torch.stack(image_embeddings) @ text_inputs.T\n",
    "similarity_scores = similarity_scores.squeeze().tolist()\n",
    "\n",
    "# Combine the image file paths with their similarity scores\n",
    "image_results = list(zip(image_files, similarity_scores))\n",
    "\n",
    "# Sort the image results based on similarity scores\n",
    "image_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the top 5 most similar images and their similarity scores\n",
    "top_k = 5\n",
    "for i in range(top_k):\n",
    "    image_path, similarity_score = image_results[i]\n",
    "    print(\"Image:\", image_path)\n",
    "    print(\"Similarity Score:\", similarity_score)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load the CLIP model and pre-trained weights\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Set the directory path to your image library\n",
    "image_directory = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\Image_classification'\n",
    "\n",
    "# Get a list of subdirectories in the image directory\n",
    "subdirectories = [\n",
    "    os.path.join(image_directory, name)\n",
    "    for name in os.listdir(image_directory)\n",
    "    if os.path.isdir(os.path.join(image_directory, name))\n",
    "]\n",
    "\n",
    "# Iterate over the subdirectories\n",
    "for subdir in subdirectories:\n",
    "    print(\"Processing subdirectory:\", subdir)\n",
    "\n",
    "    # Get a list of image files in the subdirectory\n",
    "    image_files = [\n",
    "        os.path.join(subdir, filename)\n",
    "        for filename in os.listdir(subdir)\n",
    "        if filename.endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    # Skip the subdirectory if there are no image files\n",
    "    if len(image_files) == 0:\n",
    "        print(\"No image files found in the subdirectory.\")\n",
    "        continue\n",
    "\n",
    "    # Generate the image embeddings for all the images in the subdirectory\n",
    "    image_embeddings = []\n",
    "    for image_file in image_files:\n",
    "        image = Image.open(image_file)\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.encode_image(image_input)\n",
    "        image_embeddings.append(image_embedding)\n",
    "\n",
    "    # Define the text input\n",
    "    text = \"ATP\"\n",
    "\n",
    "    # Preprocess the text input\n",
    "    text_input = clip.tokenize([text]).to(device)\n",
    "\n",
    "    # Calculate the similarity scores between the text input and image embeddings\n",
    "    similarity_scores = torch.cat(image_embeddings) @ text_input.T\n",
    "    similarity_scores = similarity_scores.squeeze().tolist()\n",
    "\n",
    "    # Find the index of the closest image\n",
    "    closest_index = similarity_scores.index(max(similarity_scores))\n",
    "\n",
    "    # Get the file name of the closest image\n",
    "    closest_image = image_files[closest_index]\n",
    "\n",
    "    # Display the result\n",
    "    print(\"Closest image:\", closest_image)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d0e040",
   "metadata": {},
   "source": [
    "#### Now let's try DALL-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the text prompt\n",
    "text_prompt = \"Glucose is phosphorylated by ATP\"\n",
    "\n",
    "# Generate the cartoon image using DALLE API\n",
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/images\",\n",
    "    headers={\n",
    "        \"Authorization\": \"Bearer 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\",\n",
    "    },\n",
    "    json={\n",
    "        \"prompt\": text_prompt,\n",
    "        \"max_tokens\": 64,\n",
    "        \"num_results\": 1,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Get the generated image URL\n",
    "image_url = response.json()[\"choices\"][0][\"finish_reason\"][\"illustration\"][\"url\"]\n",
    "\n",
    "# Download and display the cartoon image\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0de816",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "PROMPT = \"Glucose is phosphorylated by ATP\"\n",
    "\n",
    "openai.api_key = \"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\" #os.getenv()\n",
    "\n",
    "response = openai.Image.create(\n",
    "    prompt=PROMPT,\n",
    "    n=1,\n",
    "    size=\"256x256\",\n",
    ")\n",
    "\n",
    "print(response[\"data\"][0][\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df08266",
   "metadata": {},
   "source": [
    "#### Clearly OPENAI API is not the option since it generates images that are for visuals/graphic and not for cartoon. Also, it is paid version and everytime I run the code it charges me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d5711",
   "metadata": {},
   "source": [
    "## Text to image: using only text embedding and passing it through file names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cff453",
   "metadata": {},
   "source": [
    "#### Image embedding is also a reliable way to try to generate the image-image network (based on similarity score). Below are some examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c830d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define image preprocessing transform\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to generate image embedding using the pre-trained ResNet model\n",
    "def generate_image_embedding(image):\n",
    "    # Convert RGBA image to RGB\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = preprocess(image)\n",
    "\n",
    "    # Add batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "     # Convert the image tensor to the correct type\n",
    "    image = image.to(torch.long)  # Convert to torch.LongTensor\n",
    "\n",
    "    # Forward pass through the ResNet model\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "\n",
    "    # Flatten the features tensor\n",
    "    embedding = torch.flatten(features)\n",
    "\n",
    "    return embedding\n",
    "# Example usage\n",
    "#image_path = r\"C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\\ATP.png\"\n",
    "#image = Image.open(image_path)\n",
    "#image_embedding = generate_image_embedding(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6132f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the directory path to your image library\n",
    "image_directory = r\"C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\"\n",
    "\n",
    "# Process text input and generate textual embeddings\n",
    "def generate_embeddings(text):\n",
    "    encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(encoded_input)\n",
    "    return output\n",
    "\n",
    "# Calculate similarity score between two embeddings\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    # Perform similarity calculation using a chosen metric\n",
    "    # For example, you can use cosine similarity\n",
    "    similarity = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=1)\n",
    "    return similarity.item()\n",
    "\n",
    "# Generate the image embedding using a pre-trained image model\n",
    "#def generate_image_embedding(image):\n",
    "    # Implement image model here to generate the image embedding\n",
    "    # ...\n",
    "    #return image_embedding\n",
    "    \n",
    "def generate_image_embedding(image):\n",
    "    # Convert RGBA image to RGB\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = preprocess(image)\n",
    "\n",
    "    # Add batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Convert the image tensor to the correct type\n",
    "    image = image.to(torch.long)  # Convert to torch.LongTensor\n",
    "\n",
    "    # Forward pass through the GPT-2 model\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "        \n",
    "    # Flatten the features tensor\n",
    "    image_embedding = features.view(features.size(0), -1)\n",
    "\n",
    "    return image_embedding\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(encoded_input)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Find the image with the highest similarity to the input embedding\n",
    "def find_most_similar_image(input_embedding):\n",
    "    highest_similarity = -1\n",
    "    most_similar_image = None\n",
    "\n",
    "    # Iterate through the image files in the image directory\n",
    "    for filename in os.listdir(image_directory):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(image_directory, filename)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # Preprocess the image if necessary\n",
    "            # ...\n",
    "\n",
    "            # Generate the image embedding using a pre-trained image model\n",
    "            image_embedding = generate_image_embedding(image)\n",
    "\n",
    "            # Calculate similarity score between the input and image embeddings\n",
    "            similarity = calculate_similarity(input_embedding, image_embedding)\n",
    "\n",
    "            # Update the most similar image if a higher similarity is found\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                most_similar_image = image_path\n",
    "\n",
    "    return most_similar_image\n",
    "\n",
    "# Example usage\n",
    "input_text = \"ATP to phosphate\"\n",
    "input_embedding = generate_embeddings(input_text)\n",
    "most_similar_image = find_most_similar_image(input_embedding)\n",
    "\n",
    "# Display the most similar image\n",
    "if most_similar_image is not None:\n",
    "    image = Image.open(most_similar_image)\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6593cffb",
   "metadata": {},
   "source": [
    "#### The GPT-2 model is designed for language tasks and cannot directly process images. The code attempts to pass the image tensor to the GPT-2 model, which results in an error\n",
    "\n",
    "#### Let's try again, but this time using the resnet model described above and image input from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the directory path to your image library\n",
    "image_directory = r\"C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images\"\n",
    "\n",
    "# Process text input and generate textual embeddings\n",
    "def generate_embeddings(text):\n",
    "    encoded_input = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(encoded_input)\n",
    "    return output.squeeze(0)  # Remove the batch dimension\n",
    "\n",
    "# Calculate similarity score between two embeddings\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    # Reshape the tensor to match the dimensions\n",
    "    embedding1_resized = embedding1.unsqueeze(0).expand(embedding2.size(0), -1)\n",
    "    similarity = torch.nn.functional.cosine_similarity(embedding1_resized, embedding2, dim=1)\n",
    "    return similarity.item()\n",
    "\n",
    "# Generate the image embedding using a pre-trained image model (using the res net function described earlier)\n",
    "#\"\"\"\"def generate_image_embedding(image):\n",
    "    # Implement your image model here to generate the image embedding\n",
    "    # ...\n",
    "    #return image_embedding\"\"\"\"\n",
    "\n",
    "# Find the image with the highest similarity to the input embedding\n",
    "def find_most_similar_image(input_embedding):\n",
    "    highest_similarity = -1\n",
    "    most_similar_image = None\n",
    "\n",
    "    # Iterate through the image files in the image directory\n",
    "    for filename in os.listdir(image_directory):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(image_directory, filename)\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # Preprocess the image if necessary\n",
    "            # ...\n",
    "\n",
    "            # Generate the image embedding using a pre-trained image model\n",
    "            image_embedding = generate_image_embedding(image)\n",
    "\n",
    "            # Calculate similarity score between the input and image embeddings\n",
    "            similarity = calculate_similarity(input_embedding, image_embedding)\n",
    "\n",
    "            # Update the most similar image if a higher similarity is found\n",
    "            if similarity > highest_similarity:\n",
    "                highest_similarity = similarity\n",
    "                most_similar_image = image_path\n",
    "\n",
    "    return most_similar_image\n",
    "\n",
    "# Example usage\n",
    "input_text = \"ATP to phosphate\"\n",
    "input_embedding = generate_embeddings(input_text)\n",
    "most_similar_image = find_most_similar_image(input_embedding)\n",
    "\n",
    "# Display the most similar image\n",
    "if most_similar_image is not None:\n",
    "    image = Image.open(most_similar_image)\n",
    "    image.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab772152",
   "metadata": {},
   "source": [
    "#### Same issue. The calculate similarity seems to have compatibility issues with Resnet. Hence, I will have to generate a deep learning model from scratch. I have to classify the images (each images will be associated with words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07011b73",
   "metadata": {},
   "source": [
    "## Text to image using only image embedding (image classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e8ef3",
   "metadata": {},
   "source": [
    "#### Buliding a neural network based classification of the images using tensorflow. The images has been classified into nine classes: \n",
    "\n",
    "1) Cells_tissue\n",
    "2) DNA\n",
    "3) Lab_equipments\n",
    "4) Macromolecule\n",
    "5) Metabolites\n",
    "6) Organelle\n",
    "7) Parts (e.g. vegetables, flower etc.)\n",
    "8) RNA\n",
    "9) Species\n",
    "\n",
    "I did not have a validation set. But the data was split for train and test using scikitlearn. Several other issues remain in terms of curation. Species and parts as well as cell tissue images are very close to each other. Also there are some images that fell in two groups. There were some compound images as well. Let's see how the visual classification model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e128ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import images\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the root directory of your dataset\n",
    "dataset_dir = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\Image_classification'\n",
    "\n",
    "# Define the image preprocessing options\n",
    "image_generator = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # normalize pixel values to [0, 1]\n",
    "    rotation_range=20,  # randomly rotate images\n",
    "    width_shift_range=0.2,  # randomly shift images horizontally\n",
    "    height_shift_range=0.2,  # randomly shift images vertically\n",
    "    horizontal_flip=True  # randomly flip images horizontally\n",
    ")\n",
    "\n",
    "# Load the images and their labels from the directory structure\n",
    "batch_size = 32  # adjust according to your available memory\n",
    "train_generator = image_generator.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(32, 32),  # resize images to a consistent size\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'  # adjust to 'categorical' for multi-class classification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the directory path to your image dataset\n",
    "data_dir = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\Image_classification'\n",
    "\n",
    "# Set the parameters for image preprocessing and data augmentation\n",
    "image_size = (150, 150)\n",
    "batch_size = 32\n",
    "\n",
    "# Create an ImageDataGenerator for image preprocessing and data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values between 0 and 1\n",
    "    shear_range=0.2,  # Apply shear transformations\n",
    "    zoom_range=0.2,  # Apply zoom transformations\n",
    "    horizontal_flip=True  # Flip images horizontally\n",
    ")\n",
    "\n",
    "# Load images and labels from the directory\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for label_name in os.listdir(data_dir):\n",
    "    label_dir = os.path.join(data_dir, label_name)\n",
    "    if os.path.isdir(label_dir):\n",
    "        for filename in os.listdir(label_dir):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                image_path = os.path.join(label_dir, filename)\n",
    "                image_paths.append(image_path)\n",
    "                labels.append(label_name)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create generators for train and test sets\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'image_paths': train_paths, 'labels': train_labels}),\n",
    "    x_col='image_paths',\n",
    "    y_col='labels',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'image_paths': test_paths, 'labels': test_labels}),\n",
    "    x_col='image_paths',\n",
    "    y_col='labels',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Build your neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53fc9a",
   "metadata": {},
   "source": [
    "#### Lets compare other metrics for performace- F1, Recall and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\tensorflow_V1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3aca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Specify the number of images to display per class\n",
    "num_images_per_class = 6\n",
    "\n",
    "# Get the list of class names\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "# Iterate over each class\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    image_files = os.listdir(class_dir)\n",
    "    \n",
    "    # Select a random subset of images from the class\n",
    "    random_images = random.sample(image_files, num_images_per_class)\n",
    "    \n",
    "    # Display the selected images\n",
    "    for image_file in random_images:\n",
    "        image_path = os.path.join(class_dir, image_file)\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.title(class_name)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d42a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test_generator)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = np.argmax(predictions, axis=1) # argmax function well explained:https://www.geeksforgeeks.org/numpy-argmax-python/\n",
    "\n",
    "# Get a sample of test images and their corresponding labels and predicted labels\n",
    "sample_size = 20  # Number of images to display\n",
    "sample_indices = np.random.choice(range(len(test_paths)), sample_size, replace=False)\n",
    "sample_images = [test_paths[i] for i in sample_indices]\n",
    "sample_true_labels = [test_labels[i] for i in sample_indices]\n",
    "sample_predicted_labels = [class_names[predicted_labels[i]] for i in sample_indices]\n",
    "\n",
    "# Display the sample images with their true labels and predicted labels\n",
    "for image_path, true_label, predicted_label in zip(sample_images, sample_true_labels, sample_predicted_labels):\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'True Label: {true_label}\\nPredicted Label: {predicted_label}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bea87",
   "metadata": {},
   "source": [
    "#### The accuracy of the model is around 72% (one out of 4 predictions are inaccurate). Given that this was the first trial the data is encouraging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d59058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting embedding for the test data\n",
    "\n",
    "# Get the intermediate layer before the final classification layer\n",
    "embedding_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "# Extract embeddings for the test set\n",
    "test_embeddings = embedding_model.predict(test_generator)\n",
    "\n",
    "# Print the shape of the embeddings\n",
    "print(\"Embeddings shape:\", test_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39813f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting embedding for the test data\n",
    "\n",
    "# Get the intermediate layer before the final classification layer\n",
    "embedding_model_2 = tf.keras.Model(inputs=model.input, outputs=model.layers[-1].output)\n",
    "\n",
    "# Extract embeddings for the test set\n",
    "test_embeddings_2 = embedding_model_2.predict(test_generator)\n",
    "\n",
    "# Print the shape of the embeddings\n",
    "print(\"Embeddings shape:\", test_embeddings_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb7d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145c183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Reduce the dimensionality of the embeddings using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "# Get the labels for the test set\n",
    "test_labels = test_generator.classes\n",
    "\n",
    "# Plot the embeddings as a network graph\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=test_labels, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b62587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate similarity between two embeddings using cosine similarity\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    similarity = cosine_similarity(embedding1.reshape(1, -1), embedding2.reshape(1, -1))\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Create a graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "# Add nodes with corresponding embeddings\n",
    "for i, embedding in enumerate(test_embeddings):\n",
    "    graph.add_node(i, embedding=embedding)\n",
    "\n",
    "    \n",
    "# Calculate the embeddings for the images\n",
    "embeddings = model.predict(test_generator)\n",
    "    \n",
    "#Similarity matrix and threshold\n",
    "similarity_matrix = np.zeros((len(embeddings), len(embeddings)))\n",
    "for i in range(len(embeddings)):\n",
    "    for j in range(i+1, len(embeddings)):\n",
    "        similarity_matrix[i, j] = calculate_similarity(embeddings[i], embeddings[j])\n",
    "        \n",
    "threshold = np.percentile(similarity_matrix, 5)\n",
    "\n",
    "# Add edges based on similarity between embeddings\n",
    "for i in range(len(test_embeddings)):\n",
    "    for j in range(i+1, len(test_embeddings)):\n",
    "        similarity = calculate_similarity(test_embeddings[i], test_embeddings[j])\n",
    "        if similarity > threshold:  # Set a threshold for edge connections\n",
    "            graph.add_edge(i, j, weight=similarity)\n",
    "\n",
    "# Set positions for nodes using a layout algorithm\n",
    "pos = nx.spring_layout(graph)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(graph, pos, node_size=100, node_color='blue')\n",
    "\n",
    "# Draw edges\n",
    "edges = graph.edges()\n",
    "weights = [graph[u][v]['weight'] for u, v in edges]\n",
    "nx.draw_networkx_edges(graph, pos, edgelist=edges, width=weights)\n",
    "\n",
    "# Add labels for nodes (optional)\n",
    "labels = {i: str(i) for i in graph.nodes()}\n",
    "nx.draw_networkx_labels(graph, pos, labels=labels)\n",
    "\n",
    "# Show the plot\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10126085",
   "metadata": {},
   "source": [
    "#### Now applying Sentence Transformer to query this image embeddings and find out the closest image based on a text input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abadce0",
   "metadata": {},
   "source": [
    "CLIP is designed to generate image and text embeddings jointly. It learns to associate image and text representations during training, which allows it to perform tasks like image-text matching, image captioning, and zero-shot image classification.\n",
    "\n",
    "Overall, CLIP's strength lies in its ability to learn joint representations of images and texts, so it is typically used for end-to-end embedding generation rather than using external embeddings with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2315c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e261dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained Sentence Transformers model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Load your external image embeddings\n",
    "image_embeddings = load_image_embeddings(test_embeddings)\n",
    "\n",
    "# Encode your text input into a text embedding\n",
    "text_input = 'ATP'\n",
    "text_embedding = model.encode([text_input])\n",
    "\n",
    "# Compute the similarity scores between the text embedding and the image embeddings\n",
    "similarity_scores = util.cos_sim(text_embedding, image_embeddings)\n",
    "\n",
    "# Find the index of the most similar image\n",
    "most_similar_index = similarity_scores.argmax()\n",
    "\n",
    "# Retrieve the path of the most similar image based on the index\n",
    "most_similar_image_path = image_paths[most_similar_index]\n",
    "\n",
    "print(\"Most similar image:\", most_similar_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained Sentence Transformers model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Load your TensorFlow model\n",
    "tensorflow_model = tf.keras.models.load_model(r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\tensorflow_V1.h5')\n",
    "\n",
    "# Extract the image embeddings from the TensorFlow model\n",
    "image_embeddings = tensorflow_model.layers[-2].output\n",
    "\n",
    "# Encode your text input into a text embedding\n",
    "text_input = 'ATP'\n",
    "text_embedding = model.encode([text_input])\n",
    "\n",
    "# Compute the similarity scores between the text embedding and the image embeddings\n",
    "similarity_scores = util.cos_sim(text_embedding, image_embeddings)\n",
    "\n",
    "# Find the index of the most similar image\n",
    "most_similar_index = similarity_scores.argmax()\n",
    "\n",
    "# Retrieve the path of the most similar image based on the index\n",
    "most_similar_image_path = image_paths[most_similar_index]\n",
    "\n",
    "print(\"Most similar image:\", most_similar_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec27561",
   "metadata": {},
   "source": [
    "# Step 3 Using both pre-trained text and image models; to generate text-text and image-image similarity and text-image similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f42066",
   "metadata": {},
   "source": [
    "## Instead of previous step's short text input, we will now try to generate text embeddings using Wikipedia for signaling pathway (glycolysis as a starting point) and using a pdf bioscience dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d562888",
   "metadata": {},
   "source": [
    "#### We'll be using two methods. One method is using Beautifoul soup and the other one is from a PDF dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99010761",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Dictionary\\Dictionary_uptoL.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b009bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf298e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_from_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac45d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words_set = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words_set and token.isalpha()]\n",
    "    return processed_tokens\n",
    "\n",
    "processed_tokens = process_text(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenize the text data\n",
    "tokens = nltk.word_tokenize(pdf_text)\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stopwords and token.isalpha()]\n",
    "\n",
    "# Calculate word frequencies\n",
    "freq_dist = FreqDist(filtered_tokens)\n",
    "\n",
    "# Plot word frequency distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "freq_dist.plot(30, cumulative=False)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Frequency Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73760e9a",
   "metadata": {},
   "source": [
    "#### Word relationship in terms of what an word is followed by can be generated as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_relationships(tokens):\n",
    "    relationships = defaultdict(list)\n",
    "    for i in range(len(tokens) - 1):\n",
    "        relationships[tokens[i]].append(tokens[i+1])\n",
    "    return relationships\n",
    "\n",
    "word_relationships = generate_word_relationships(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f24280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_word_relationships(word_relationships):\n",
    "    # Create an empty directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges to the graph based on word relationships\n",
    "    for word, next_words in word_relationships.items():\n",
    "        for next_word in next_words:\n",
    "            G.add_edge(word, next_word)\n",
    "\n",
    "    # Set up the layout for the graph\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    # Draw the nodes and edges\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray')\n",
    "\n",
    "    # Add labels to the nodes\n",
    "    nx.draw_networkx_labels(G, pos, font_color='black', font_size=10, font_weight='bold')\n",
    "\n",
    "    # Adjust the figure size and display the graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Relationships')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you already have the word relationships dictionary 'word_relationships'\n",
    "visualize_word_relationships(word_relationships)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f5aab",
   "metadata": {},
   "source": [
    "#### Clearing up the network with 30 top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44517b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def visualize_word_relationships(word_relationships):\n",
    "    # Get the most prominent 30 nodes based on their connections\n",
    "    most_prominent_nodes = Counter(word_relationships).most_common(30)\n",
    "    most_prominent_nodes = [node for node, _ in most_prominent_nodes]\n",
    "\n",
    "    # Create a subgraph with only the most prominent nodes and their connections\n",
    "    subgraph = {node: word_relationships[node] for node in most_prominent_nodes}\n",
    "\n",
    "    # Create an empty directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges to the graph based on the subgraph\n",
    "    for word, next_words in subgraph.items():\n",
    "        for next_word in next_words:\n",
    "            G.add_edge(word, next_word)\n",
    "\n",
    "    # Set up the layout for the graph\n",
    "    pos = nx.spring_layout(G)\n",
    "\n",
    "    # Draw the nodes and edges\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=most_prominent_nodes, node_color='lightblue', node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray')\n",
    "\n",
    "    # Add labels to the nodes\n",
    "    nx.draw_networkx_labels(G, pos, font_color='black', font_size=10, font_weight='bold')\n",
    "\n",
    "    # Adjust the figure size and display the graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Relationships (Top 30 Nodes)')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you already have the word relationships dictionary 'word_relationships'\n",
    "visualize_word_relationships(word_relationships)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e0d90",
   "metadata": {},
   "source": [
    "#### Using beutiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d52cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea43bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Glycolysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a648b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb4a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.find(id=\"mw-content-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be61466",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = content.find_all(\"p\")\n",
    "for paragraph in paragraphs:\n",
    "    text = paragraph.get_text()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code with the result saved in a variable\n",
    "paragraph_texts = []  # New variable to store the extracted texts\n",
    "paragraphs = content.find_all(\"p\")\n",
    "for paragraph in paragraphs:\n",
    "    text = paragraph.get_text().lower()\n",
    "    paragraph_texts.append(text)  # Append the text to the new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286770da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of paragraph texts into a single string\n",
    "text_string = \" \".join(paragraph_texts)\n",
    "\n",
    "# Now you can use the text_string variable as a single string containing all the extracted text\n",
    "print(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5559692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def calculate_word_frequency(text_string):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text_string = re.sub(r'[^\\w\\s]', '', text_string.lower())\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text_string.split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "# Example usage\n",
    "frequency = calculate_word_frequency(text_string)\n",
    "print(frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b755c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def visualize_word_frequency_wordcloud(word_freq):\n",
    "    wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(word_freq)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Frequency WordCloud')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you already have the word frequency dictionary 'frequency'\n",
    "visualize_word_frequency_wordcloud(frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd1a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_word_frequency_bar_chart(word_freq):\n",
    "    words = list(word_freq.keys())\n",
    "    frequencies = list(word_freq.values())\n",
    "\n",
    "    plt.bar(words, frequencies)\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Word Frequency Bar Chart')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you already have the word frequency dictionary 'frequency'\n",
    "visualize_word_frequency_bar_chart(frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccafc4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_word_frequency_bar_chart(word_freq):\n",
    "    top_words = sorted(word_freq, key=word_freq.get, reverse=True)[:20]\n",
    "    frequencies = [word_freq[word] for word in top_words]\n",
    "\n",
    "    plt.bar(top_words, frequencies)\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 20 Word Frequency Bar Chart')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you already have the word frequency dictionary 'frequency'\n",
    "visualize_word_frequency_bar_chart(frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06558e",
   "metadata": {},
   "source": [
    "#### Word meaning similarity between the texts can be idnetified using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ceecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text_string\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c3b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Word2Vec model on your text data\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access the word vectors for specific words\n",
    "word_vector = model.wv['ATP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Prepare your text input\n",
    "text = text_string\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Train a Word2Vec model on your text data\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Measure similarity between words\n",
    "word_pairs = [(\"atp\", \"phosphate\"), (\"glucose\", \"glucose-6-phosphate\"), (\"aerobic\", \"anaerobic\")]\n",
    "\n",
    "for pair in word_pairs:\n",
    "    word1, word2 = pair\n",
    "    if word1 in model.wv.key_to_index and word2 in model.wv.key_to_index:\n",
    "        similarity = model.wv.similarity(word1, word2)\n",
    "        print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "    else:\n",
    "        print(f\"One or both of the words '{word1}' and '{word2}' are not present in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b16b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare text input\n",
    "text = text_string\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Train a Word2Vec model on text data\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get word vectors and corresponding words\n",
    "word_vectors = model.wv.vectors\n",
    "words = model.wv.index_to_key\n",
    "\n",
    "# Perform clustering\n",
    "num_clusters = 3  # Specify the number of clusters \n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(word_vectors)\n",
    "\n",
    "# Apply dimensionality reduction for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(pca_result[i, 0], pca_result[i, 1], c=colors[kmeans.labels_[i]])\n",
    "    plt.annotate(word, xy=(pca_result[i, 0], pca_result[i, 1]), xytext=(5, 2),\n",
    "                 textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.title('Word Clustering')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b950d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "# Prepare text input\n",
    "text = text_string\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Train a Word2Vec model on text data\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get word vectors and corresponding words\n",
    "word_vectors = model.wv.vectors\n",
    "words = model.wv.index_to_key\n",
    "\n",
    "# Perform clustering\n",
    "num_clusters = 3  # Specify the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(word_vectors)\n",
    "\n",
    "# Apply dimensionality reduction for visualization\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Select the top 30 most frequent nodes\n",
    "word_freq = Counter([word for sentence in tokenized_sentences for word in sentence])\n",
    "top_nodes = [word for word, _ in word_freq.most_common(30)]\n",
    "\n",
    "# Plot the clusters for the top nodes\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "for i, word in enumerate(words):\n",
    "    if word in top_nodes:\n",
    "        plt.scatter(pca_result[i, 0], pca_result[i, 1], c=colors[kmeans.labels_[i]])\n",
    "        plt.annotate(word, xy=(pca_result[i, 0], pca_result[i, 1]), xytext=(5, 2),\n",
    "                     textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.title('Word Clustering (Top 30 Nodes)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44942c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97341c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Prepare text input\n",
    "text = text_string\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8c2f5",
   "metadata": {},
   "source": [
    "#### Now using multi-model fusion to merge NLP and image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf234809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple fusion model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, image_embedding_dim, fusion_dim):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.fc = nn.Linear(word_embedding_dim + image_embedding_dim, fusion_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, word_embedding, image_embedding):\n",
    "        # Concatenate word and image embeddings\n",
    "        fusion_input = torch.cat((word_embedding, image_embedding), dim=1)\n",
    "        \n",
    "        # Fusion layer\n",
    "        fused_embedding = self.fc(fusion_input)\n",
    "        fused_embedding = self.relu(fused_embedding)\n",
    "        \n",
    "        return fused_embedding\n",
    "\n",
    "# Example usage\n",
    "word_embedding_dim = 300\n",
    "image_embedding_dim = 512\n",
    "fusion_dim = 256\n",
    "\n",
    "word_embedding = torch.randn(1, word_embedding_dim)  # Replace with actual word embedding\n",
    "image_embedding = torch.randn(1, image_embedding_dim)  # Replace with actual image embedding\n",
    "\n",
    "# Create fusion model\n",
    "fusion_model = FusionModel(word_embedding_dim, image_embedding_dim, fusion_dim)\n",
    "\n",
    "# Forward pass through the fusion model\n",
    "fused_embedding = fusion_model(word_embedding, image_embedding)\n",
    "\n",
    "print(fused_embedding.shape)  # Shape of the fused embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b11dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = text_embedding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple fusion model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, image_embedding_dim, fusion_dim):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.fc = nn.Linear(word_embedding_dim + image_embedding_dim, fusion_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, word_embedding, image_embedding):\n",
    "        # Concatenate word and image embeddings\n",
    "        fusion_input = torch.cat((word_embedding, image_embedding), dim=1)\n",
    "        \n",
    "        # Fusion layer\n",
    "        fused_embedding = self.fc(fusion_input)\n",
    "        fused_embedding = self.relu(fused_embedding)\n",
    "        \n",
    "        return fused_embedding\n",
    "\n",
    "# Example usage\n",
    "word_embedding_dim = 300\n",
    "image_embedding_dim = 512\n",
    "fusion_dim = 256\n",
    "\n",
    "word_embedding = torch.randn(1, word_embedding_dim)  # Replace with actual word embedding\n",
    "image_embedding = torch.randn(1, image_embedding_dim)  # Replace with actual image embedding\n",
    "\n",
    "# Create fusion model\n",
    "fusion_model = FusionModel(word_embedding_dim, image_embedding_dim, fusion_dim)\n",
    "\n",
    "# Forward pass through the fusion model\n",
    "fused_embedding = fusion_model(word_embedding, image_embedding)\n",
    "\n",
    "print(fused_embedding.shape)  # Shape of the fused embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c60e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (word_embedding.shape)\n",
    "print (image_embedding.shape)\n",
    "print (fused_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d92a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings #this cannot be used for the fused model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20157ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for how to convert keras tensor to torch tensor\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming `keras_tensor` is your Keras tensor stored as a NumPy array\n",
    "keras_tensor = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Convert Keras tensor to PyTorch tensor\n",
    "torch_tensor = torch.from_numpy(keras_tensor)\n",
    "\n",
    "print(\"Keras Tensor:\")\n",
    "print(keras_tensor)\n",
    "print(\"PyTorch Tensor:\")\n",
    "print(torch_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained Sentence Transformers model\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "\n",
    "# Encode your text input into a text embedding\n",
    "\n",
    "text_embedding = model.encode([text_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0aab69ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(713, 128)\n",
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "print (test_embeddings.shape)\n",
    "print (text_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "68f61f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        0.        2.7548897 ... 0.        0.        6.1208005]]\n",
      "(1, 91264)\n"
     ]
    }
   ],
   "source": [
    "test_embeddings_reshaped = test_embeddings.reshape(1, -1)\n",
    "print (test_embeddings_reshaped)\n",
    "print(test_embeddings_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4ffd7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 896])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple fusion model\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, image_embedding_dim, fusion_dim):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.fc = nn.Linear(word_embedding_dim + image_embedding_dim, fusion_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, word_embedding, image_embedding):\n",
    "        # Concatenate word and image embeddings\n",
    "        fusion_input = torch.cat((word_embedding, image_embedding), dim=1)\n",
    "        \n",
    "        # Fusion layer\n",
    "        fused_embedding = self.fc(fusion_input)\n",
    "        fused_embedding = self.relu(fused_embedding)\n",
    "    \n",
    "        return fused_embedding\n",
    "\n",
    "# Example usage\n",
    "word_embedding_dim = 768\n",
    "image_embedding_dim = 91264\n",
    "fusion_dim = 896\n",
    "\n",
    "word_embedding = torch.from_numpy(text_embedding)\n",
    "image_embedding = torch.from_numpy(test_embeddings_reshaped)\n",
    "\n",
    "# Create fusion model\n",
    "fusion_model = FusionModel(word_embedding_dim, image_embedding_dim, fusion_dim)\n",
    "\n",
    "# Forward pass through the fusion model\n",
    "fused_embedding = fusion_model(word_embedding, image_embedding)\n",
    "\n",
    "print(fused_embedding.shape)  # Shape of the fused embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9918ca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the fused embedding tensor to a numpy array\n",
    "fused_embedding_array = fused_embedding.detach().numpy()\n",
    "\n",
    "# Plot the fused embedding\n",
    "plt.plot(fused_embedding_array[0])  # Assuming batch size is 1\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Fused Embedding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4eda2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aubhi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aubhi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dbc2157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 896]      82,461,568\n",
      "              ReLU-2                  [-1, 896]               0\n",
      "================================================================\n",
      "Total params: 82,461,568\n",
      "Trainable params: 82,461,568\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 267.38\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 314.57\n",
      "Estimated Total Size (MB): 581.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate a network visualization\n",
    "from torchsummary import summary\n",
    "summary(fusion_model, [(word_embedding_dim,), (image_embedding_dim,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9309d001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchviz==0.0.2\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from torchviz==0.0.2) (2.0.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from torchviz==0.0.2) (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from torch->torchviz==0.0.2) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from torch->torchviz==0.0.2) (3.10.0.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from torch->torchviz==0.0.2) (1.9)\n",
      "Requirement already satisfied: networkx in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from torch->torchviz==0.0.2) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from torch->torchviz==0.0.2) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from jinja2->torch->torchviz==0.0.2) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\aubhi\\anaconda3\\lib\\site-packages (from sympy->torch->torchviz==0.0.2) (1.2.1)\n",
      "Building wheels for collected packages: torchviz\n",
      "  Building wheel for torchviz (setup.py): started\n",
      "  Building wheel for torchviz (setup.py): finished with status 'done'\n",
      "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4151 sha256=4dca3995888de516396ea87df992fd15b2d0dbd92a663ffc2ef0a9ce54b3c3c2\n",
      "  Stored in directory: c:\\users\\aubhi\\appdata\\local\\pip\\cache\\wheels\\29\\65\\6e\\db2515eb1dc760fecd36b40d54df65c1e18534013f1c037e2e\n",
      "Successfully built torchviz\n",
      "Installing collected packages: torchviz\n",
      "Successfully installed torchviz-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aubhi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\aubhi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install torchviz==0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a network visualization\n",
    "from torchviz import make_dot\n",
    "dot = make_dot(fusion_model(word_embedding, image_embedding), params=dict(fusion_model.named_parameters()))\n",
    "dot.render('fusion_model', format='png')  # Save the visualization as an image file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92948612",
   "metadata": {},
   "source": [
    "#### Now we can input the fused model as an input to an image generation model. I am training a GAN based model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, img_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, img_size * img_size * img_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        img = self.net(z)\n",
    "        img = img.view(-1, img_channels, img_size, img_size)\n",
    "        return img\n",
    "\n",
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(img_size * img_size * img_channels, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img = img.view(-1, img_size * img_size * img_channels)\n",
    "        validity = self.net(img)\n",
    "        return validity\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_channels = 3\n",
    "img_size = 64\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Prepare the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = ImageFolder(r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\Image_classification', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(latent_dim, img_channels, img_size)\n",
    "discriminator = Discriminator(img_channels, img_size)\n",
    "\n",
    "# Define loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(real_images.size(0), 1)\n",
    "        fake = torch.zeros(real_images.size(0), 1)\n",
    "        \n",
    "        # Train generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        z = torch.randn(real_images.size(0), latent_dim)\n",
    "        generated_images = generator(z)\n",
    "        generator_loss = adversarial_loss(discriminator(generated_images), valid)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        # Train discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(real_images), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_images.detach()), fake)\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Generator Loss: {generator_loss.item():.4f}, Discriminator Loss: {discriminator_loss.item():.4f}\")\n",
    "    \n",
    "    # Save generated images\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        z = torch.randn(10, latent_dim)\n",
    "        generated_images = generator(z)\n",
    "        save_image(generated_images, f\"generated_images_epoch_{epoch+1}.png\", nrow=10, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca0650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Display generated images\n",
    "generated_images = generated_images.detach().cpu()\n",
    "grid = torchvision.utils.make_grid(generated_images, nrow=10, normalize=True)\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bd0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, img_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, img_size * img_size * img_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        img = self.net(z)\n",
    "        img = img.view(-1, img_channels, img_size, img_size)\n",
    "        return img\n",
    "\n",
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(img_size * img_size * img_channels, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img = img.view(-1, img_size * img_size * img_channels)\n",
    "        validity = self.net(img)\n",
    "        return validity\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_channels = 3\n",
    "img_size = 64\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Prepare the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = ImageFolder(r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\Image_classification', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(latent_dim, img_channels, img_size)\n",
    "discriminator = Discriminator(img_channels, img_size)\n",
    "\n",
    "# Define loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(real_images.size(0), 1)\n",
    "        fake = torch.zeros(real_images.size(0), 1)\n",
    "        \n",
    "        # Train generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        z = torch.randn(real_images.size(0), latent_dim)\n",
    "        generated_images = generator(z)\n",
    "        generator_loss = adversarial_loss(discriminator(generated_images), valid)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        # Train discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(real_images), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_images.detach()), fake)\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Generator Loss: {generator_loss.item():.4f}, Discriminator Loss: {discriminator_loss.item():.4f}\")\n",
    "    \n",
    "    # Save generated images\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        z = torch.randn(10, latent_dim)\n",
    "        generated_images = generator(z)\n",
    "        save_image(generated_images, f\"generated_images_epoch_{epoch+1}.png\", nrow=10, normalize=True)\n",
    "\n",
    "       \n",
    "        # Rescale images from [-1, 1] to [0, 1] for visualization\n",
    "        generated_images = (generated_images + 1) / 2\n",
    "        \n",
    "        # Create a grid of generated images\n",
    "        grid = vutils.make_grid(generated_images, nrow=5, normalize=True)\n",
    "        \n",
    "        # Display the grid of generated images\n",
    "        plt.imshow(grid.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c4e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Define the generator and other parts of the code\n",
    "\n",
    "# Generate and collect embeddings\n",
    "num_images = 100  # Number of generated images to visualize\n",
    "embedding_dim = 256  # Dimensionality of the image embeddings\n",
    "\n",
    "embeddings = []\n",
    "for _ in range(num_images):\n",
    "    z = torch.randn(1, latent_dim)\n",
    "    generated_image = generator(z)\n",
    "    generated_image = generated_image.view(-1, embedding_dim).detach().cpu().numpy()\n",
    "    embeddings.append(generated_image)\n",
    "\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Visualize t-SNE embeddings\n",
    "plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1])\n",
    "plt.title('t-SNE Visualization of Generated Image Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16eea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, img_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, img_size * img_size * img_channels),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        img = self.net(z)\n",
    "        img = img.view(-1, img_channels, img_size, img_size)\n",
    "        return img\n",
    "\n",
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, img_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(img_size * img_size * img_channels, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        img = img.view(-1, img_size * img_size * img_channels)\n",
    "        validity = self.net(img)\n",
    "        return validity\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_channels = 3\n",
    "img_size = 64\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Prepare the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "dataset = ImageFolder(r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\Image_classification', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator(latent_dim, img_channels, img_size)\n",
    "discriminator = Discriminator(img_channels, img_size)\n",
    "\n",
    "# Define loss function and optimizers\n",
    "adversarial_loss = nn.BCELoss()\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones(real_images.size(0), 1)\n",
    "        fake = torch.zeros(real_images.size(0), 1)\n",
    "        \n",
    "        # Train generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        z = torch.randn(real_images.size(0), latent_dim)\n",
    "        generated_images = generator(z)\n",
    "        generator_loss = adversarial_loss(discriminator(generated_images), valid)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        \n",
    "        # Train discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(real_images), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_images.detach()), fake)\n",
    "        discriminator_loss = (real_loss + fake_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        # Print training progress\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Generator Loss: {generator_loss.item():.4f}, Discriminator Loss: {discriminator_loss.item():.4f}\")\n",
    "    \n",
    "    # Save generated images\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        z = torch.randn(10, latent_dim)\n",
    "        generated_images = generator(z)\n",
    "        save_image(generated_images, f\"generated_images_epoch_{epoch+1}.png\", nrow=10, normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bd57d1",
   "metadata": {},
   "source": [
    "#### The code needs to be revised. This was just a trial. We need to import appropriate text from Wikipedia. The wikipedia package can be imported. Then We'll have to redo the text embeddings. For generating the text embeddings prepositions were also not oberlooked and looking at the PCA analysis it was the prepositions (such as 'the') were the ones that were least clustered.\n",
    "\n",
    "Also this was done via GAN; we can also try VAE to see if that improves. \n",
    "\n",
    "The next steps would be to use file names as the text embeddings and then concatenate the text embeddings with image embeddings. Run the decoder (either GLIDE for VAE or GAN as used here). Will be giving it a shot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c302727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Step 3: Define the FusionModel class\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, text_embedding_dim, image_embedding_dim, fusion_dim):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.fc = nn.Linear(text_embedding_dim + image_embedding_dim, fusion_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, text_embedding, image_embedding):\n",
    "        # Concatenate text and image embeddings\n",
    "        fusion_input = torch.cat((text_embedding, image_embedding), dim=1)\n",
    "\n",
    "        # Fusion layer\n",
    "        fused_embedding = self.fc(fusion_input)\n",
    "        fused_embedding = self.relu(fused_embedding)\n",
    "\n",
    "        return fused_embedding\n",
    "\n",
    "\n",
    "# Step 5: Load the image library and extract the image embedding\n",
    "image_dir = r'C:\\Users\\aubhi\\OneDrive - University of California, San Francisco\\Desktop\\Biorender\\All_images'  # Replace with the path to your image directory\n",
    "image_embedding_dim = 2048  # Set the desired image embedding dimension\n",
    "\n",
    "# Load the image embedding model (ResNet50)\n",
    "image_model = resnet50(pretrained=True)\n",
    "image_model.fc = nn.Identity()  # Remove the last fully connected layer\n",
    "image_model.eval()\n",
    "\n",
    "# Step 6: Tokenize the file names and obtain the text embeddings\n",
    "file_names = os.listdir(image_dir)\n",
    "\n",
    "# Tokenize the file names\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs_list = []\n",
    "\n",
    "image_embeddings = []  # List to store the image embeddings\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(image_dir, file_name)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    image = Image.open(file_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Obtain the image embedding\n",
    "    with torch.no_grad():\n",
    "        image_embedding = image_model(image).squeeze()\n",
    "\n",
    "    # Tokenize the file name\n",
    "    inputs = tokenizer.encode_plus(file_name, add_special_tokens=True, padding='max_length', max_length=256, truncation=True, return_tensors='pt')\n",
    "    inputs_list.append(inputs)\n",
    "\n",
    "# Load the text embedding model\n",
    "text_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "text_model.eval()\n",
    "\n",
    "# Obtain the text embeddings\n",
    "text_embeddings = []\n",
    "\n",
    "for inputs in inputs_list:\n",
    "    with torch.no_grad():\n",
    "        text_embedding = text_model(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
    "        text_embeddings.append(text_embedding)\n",
    "\n",
    "# Step 7: Instantiate the fusion model and generate the fused embeddings\n",
    "text_embedding_dim = text_model.config.hidden_size  # Get the dimension of the text embedding\n",
    "fusion_dim = 256  # Set the desired dimension of the fused embedding\n",
    "\n",
    "# Create the fusion model\n",
    "fusion_model = FusionModel(text_embedding_dim, image_embedding_dim, fusion_dim)\n",
    "\n",
    "fused_embeddings = []\n",
    "\n",
    "# Generate the fused embeddings\n",
    "for text_embedding, image_embedding in zip(text_embeddings, image_embeddings):\n",
    "    fused_embedding = fusion_model(text_embedding, image_embedding)\n",
    "    fused_embeddings.append(fused_embedding)\n",
    "\n",
    "# Step 8: Use the fused embeddings for further processing or training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb78d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a40925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Step 8: Use the fused embeddings for image generation\n",
    "# Define the ImageGeneratorModel class\n",
    "class ImageGeneratorModel(nn.Module):\n",
    "    def __init__(self, fused_embedding_dim, image_channels, image_size):\n",
    "        super(ImageGeneratorModel, self).__init__()\n",
    "        self.fc = nn.Linear(fused_embedding_dim, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, image_size * image_size * image_channels)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def forward(self, fused_embedding):\n",
    "        x = self.fc(fused_embedding)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = x.view(-1, self.image_channels, self.image_size, self.image_size)\n",
    "        return x\n",
    "\n",
    "# Set the dimensions and size of the generated image\n",
    "image_channels = 3  # RGB channels\n",
    "image_size = 64  # Image size in pixels\n",
    "\n",
    "# Load the image generator model\n",
    "image_generator = ImageGeneratorModel(fusion_dim, image_channels, image_size)\n",
    "image_generator.eval()\n",
    "\n",
    "# Generate images from fused embeddings\n",
    "generated_images = []\n",
    "\n",
    "for fused_embedding in fused_embeddings:\n",
    "    with torch.no_grad():\n",
    "        generated_image = image_generator(fused_embedding.unsqueeze(0))\n",
    "        generated_images.append(generated_image)\n",
    "\n",
    "# Step 9: Further processing or visualization of the generated images\n",
    "# You can perform additional processing or visualization with the generated images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 9: Further processing or visualization of the generated images\n",
    "for i, generated_image in enumerate(generated_images):\n",
    "    # Convert the tensor to numpy array\n",
    "    generated_image_np = generated_image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    # Display the generated image\n",
    "    plt.imshow(generated_image_np)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Generated Image {i+1}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 9: Visualize the fused embeddings as a heatmap\n",
    "fused_embeddings_np = torch.stack(fused_embeddings).detach().numpy()\n",
    "\n",
    "# Calculate similarity matrix using cosine similarity\n",
    "similarity_matrix = np.dot(fused_embeddings_np, fused_embeddings_np.T)\n",
    "similarity_matrix = similarity_matrix / (np.linalg.norm(fused_embeddings_np, axis=1)[:, np.newaxis] * np.linalg.norm(fused_embeddings_np, axis=1))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Fused Embeddings Similarity Heatmap')\n",
    "plt.xlabel('Instance ID')\n",
    "plt.ylabel('Instance ID')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c00f4",
   "metadata": {},
   "source": [
    "### For Voice recognition; Things to do next\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "#### Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#### Define the training data\n",
    "train_dataset = torchaudio.datasets.SpeechCommands(root=\"./data\", download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#### Initialize the neural network\n",
    "net = Net()\n",
    "\n",
    "#### Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "#### Train the neural network\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "#### Test the neural network\n",
    "test_dataset = torchaudio.datasets.SpeechCommands(root=\"./data\", download=True, subset=\"testing\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test set: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5bf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
